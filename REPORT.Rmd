---
title: "Bitcoin candlestick predictions using lagged features and machine learning algorithm in R"
subtitle: "Training various machine learning algorithms to predict the next candlestick of the bitcoin price using various lagged features"
abstract: "This report explores how to get the best accuracy on predicting the next candlestick of the bitcoin chart using the previous ones. It compares different algorithms: Generalized Linear Model, Decision Tree, Random Forest, KNN and Gradient boosting and different number of lagged features. This project is part of the 'Data Science: Capstone' module of HarvardX PH125.9x from the edx platform."
author: "Sandoche Adittane"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts = list(width.cutoff = 60), tidy = TRUE)
  
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(formatR)) install.packages("formatR", repos = "http://cran.us.r-project.org")
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if (!require(httr)) install.packages("httr", repos = "http://cran.us.r-project.org")
if (!require(jsonlite)) install.packages("jsonlite", repos = "http://cran.us.r-project.org")
if (!require(tidyquant)) install.packages("tidyquant", repos = "http://cran.us.r-project.org")
if (!require(patchwork)) install.packages("patchwork", repos = "http://cran.us.r-project.org")
if (!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if (!require(TTR)) install.packages("TTR", repos = "http://cran.us.r-project.org")
if (!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")

library(lubridate)
library(formatR)
library(tidyverse)
library(caret)
library(httr)
library(jsonlite)
library(tidyquant)
library(patchwork)
library(randomForest)
library(TTR)
library(gbm)
```

\newpage
\tableofcontents
\listoffigures
\listoftables
\newpage

# Overview

In this study we will try to predict the direction of the next candlestick of the bitcoin chart. Before starting, it's important to understand what are Bitcoin, candlesticks and what is the goal of this study.

## Introduction to Bitcoin

This last years Bitcoin (BTC) has been gaining attention not only by retail investors but also by institutional investor. In 2025 we've seen the emergence of spot Bitcoin exchange-traded funds (ETF) from institutions such as BlackRock, VanEck, Grayscale. With a market capitalization of about 1.68 billion in dollars at the time of writing, Bitcoin started as a peer-to-peer currency, a free alernative to centralized currencies controlled by central banks. It is now used more as an investment, a store of value and even considered as a strategic reserve assets by some countries. 

TODO: Add examples with sources.

Bitcoin ows is decentralization and  to it's data structure, the blockchain, a chain of block that contains transaction, and to its consensus, the proof of work. Without going too much into details, it makes a Bitcoin a currency that does not rely on a centralized server. Proof of work is a cryptographic competition where the Bitcoin servers called nodes compete to decide which one is the next block to be added to the blockchain. They go through a process called mining where nodes have to use their computing power to find a number called nonce. This computing power is called the hashrate. The node who succeed at "mining" successfully gets rewarded for that.

TODO: reference to my article

The fact that Bitcoin is defined by its codebase is quite facinating, also having all its ledger visible and publically available gives a lot of data available to analyze. Moreover unlike stocks BTC can be traded any time, there is no opening or closing hours, the bitcoin market never stops and it is very easy for anyone to buy and sell bitcoin. Those are two reasons worth studying bitcoin's candlestick charts instead of other asset.

## What are candlesticks?

Let's talk about the candlestick. The price of assets such as bitcoin is described by a serie of candle stick defined by, an opening price, a close price a high and a low also called OHLC. A candlestick can be "up" / "bullish" if closing price is higher than opening price, or "down" / "bearish" otherwise. You can see this visually with the following figure. ""

https://i0.wp.com/techqualitypedia.com/wp-content/uploads/2024/09/candlestick-components.jpg?w=1491&ssl=1
Source: https://techqualitypedia.com/candlestick-patterns-bullish/

The candle stick chart is defined as a time serie of candles, each candle is defined at a defined time and have a time duration. We will explain more in detail in the exploratory analysis.

## Candlesticks pattern

TODO Talk about chartists and common patterns

## Goal of the study

The goal of the study is to find a model able to predict the direction of a candlestick using N previous candles. This number N will be also part of the research. We will have to not only find N but also find what are the best features to achieve the best accuracy.

## Applications

Why is the direction of a candlestick matter? Because being able to predict the direction of the next candle could enable trader to buy and sell on spot market when the predicted candle is green. Also perpetual futures trader can go both way, they can long when the prediction says "up" and "short" when the predictions says "down".

TODO: Give some resource to learn about spot vs future.

# Exploratory data analysis

In this section we will see what are the are the different dataset available, see what features are available to train the different models, prepare the data, verify it, and choose different machine learning algorithms we will use and compare.

## Data sets

In order to conduct this study we used as a the main data set the historic rates for the trading pair BTC-USD using Coinbase API.
TODO: add reference https://docs.cdp.coinbase.com/exchange/reference/exchangerestapi_getproductcandles

We used the following global variables for the full project:
```{r global_variables}
trading_pair <- "BTC-USD"
start_date <- "2024-01-01"
end_date <- "2025-03-29"
candlestick_period <- 3600
set.seed(1)
```

The timeframe is the entire year 2024 and the start of the year 2025 until the day we started the study. Note that since January 2024, Bitcoin ETF has officially been approved. The period `candlestick_period <- 3600` is the time of a candle, the candle closes 1h after it starts. Which means we have 24 candles per day.

I choose this settings to have a dataset of around 10000 candles but also since Bitcoin ETF has been approved the market may have taken a different dynamic than the previous years.

Let's see how the dataset looks like.

```{r btc_coinbase_candlestick_chart, cache=TRUE, echo=FALSE}
candles <- read_csv(paste0("data/", trading_pair, "_candles_", start_date, "_", end_date, "_", candlestick_period, ".csv"), show_col_types = FALSE)
knitr::kable(head(candles), format = "simple", caption = "Overview of the BTC-USD candlestick dataset")
```

We have `r format(nrow(candles), big.mark=",")` entries in our candle stick dataset. As described in the overview it contains the OCLH data, timestamp and the volume of each candles.

Bitcoin is used by 3 types of users: 

- Traders — they are interested by the price and make profit

- Users — using the currency to do payments or to transfer money around the world

- Miners — they mine bitcoin to sell it, their interest is that the price of bitcoin is higher than the cost of mining bitcoin


Keeping this in mind, I tried to find other dataset that could represent each of the type of users that could eventually help in our predictions and I picked the following:

- Fear and greed index — represents the overall mood of the market (traders)

- Hash-rate — defines the overall mining power (miners)

- Average block size — the higher it is the more transactions are happening (users)

- Number of transactions — defines the activity of the network (users)

- Number of unspent transaction outputs (UTXO) — defines how many addresses contains bitcoin, and reflects the network activity (users)

 
https://www.blockchain.com/explorer/charts/total-bitcoins
https://alternative.me/crypto/fear-and-greed-index/

```{r fear_and_greed_index, cache=TRUE}
fear_and_greed_index <- read_csv(paste0("data/", trading_pair, "_fear_and_greed_index_", start_date, "_", end_date, ".csv"))
fear_and_greed_index <- fear_and_greed_index %>% mutate(value = as.numeric(value))
knitr::kable(head(fear_and_greed_index), format = "simple", caption = "Overview of the BTC fear and greed index dataset")
```

This dataset is a time serie of the daily fear and greed index, it is a value between 0 and 100, 0 being the most fearful and 100 being the most greedy.
The data set contains `r nrow(fear_and_greed_index)` entries.

```{r hash_rate, cache=TRUE}
hash_rate <- jsonlite::fromJSON("data/hash-rate.json")$`hash-rate` %>%
  rename(timestamp = x, hash_rate = y) %>%
  mutate(timestamp = as.POSIXct(timestamp / 1000, origin = "1970-01-01", tz = "UTC")) %>%
  filter(timestamp >= as.POSIXct(start_date, origin = "1970-01-01", tz = "UTC") & timestamp <= as.POSIXct(end_date, origin = "1970-01-01", tz = "UTC"))
knitr::kable(head(hash_rate), format = "simple", caption = "Overview of the BTC hash rate dataset")
```

This dataset is a time serie of the daily hash rate, it is a value in TH/s.
The data set contains `r nrow(hash_rate)` entries.

```{r average_block_size, cache=TRUE}
average_block_size <- jsonlite::fromJSON("data/avg-block-size.json")$`avg-block-size` %>%
  rename(timestamp = x, avg_block_size = y) %>%
  mutate(timestamp = as.POSIXct(timestamp / 1000, origin = "1970-01-01", tz = "UTC")) %>%
  filter(timestamp >= as.POSIXct(start_date, origin = "1970-01-01", tz = "UTC") & timestamp <= as.POSIXct(end_date, origin = "1970-01-01", tz = "UTC"))
knitr::kable(head(average_block_size), format = "simple", caption = "Overview of the BTC average block size dataset")
```

This dataset is a time serie of the daily average block size, it is a value in bytes.
The data set contains `r nrow(average_block_size)` entries.

```{r n_transactions, cache=TRUE}
n_transactions <- jsonlite::fromJSON("data/n-transactions.json")$`n-transactions` %>%
  rename(timestamp = x, n_transactions = y) %>%
  mutate(timestamp = as.POSIXct(timestamp / 1000, origin = "1970-01-01", tz = "UTC")) %>%
  filter(timestamp >= as.POSIXct(start_date, origin = "1970-01-01", tz = "UTC") & timestamp <= as.POSIXct(end_date, origin = "1970-01-01", tz = "UTC"))
knitr::kable(head(n_transactions), format = "simple", caption = "Overview of the BTC number of transactions dataset")
```

This dataset is a time serie of the daily number of transactions, it is a value in transactions.
The data set contains `r nrow(n_transactions)` entries.


```{r utxo_count, cache=TRUE}
utxo_count <- jsonlite::fromJSON("data/utxo-count.json")$`utxo-count` %>%
  rename(timestamp = x, utxo_count = y) %>%
  mutate(
    timestamp = as.POSIXct(timestamp / 1000, origin = "1970-01-01", tz = "UTC"),
    timestamp = as.Date(timestamp)
  ) %>%
  filter(timestamp >= as.Date(start_date) & timestamp <= as.Date(end_date)) %>%
  group_by(timestamp) %>%
  summarise(utxo_count = mean(utxo_count)) # Take average for each date
knitr::kable(head(utxo_count), format = "simple", caption = "Overview of the BTC UTXO count dataset")
```

This dataset is a time serie of the daily number of UTXO, it is the count of UTXO in the network.
The data set contains `r nrow(utxo_count)` entries. We had to group by timestamp since some dates had more than 1 value.

As we can see `fear_and_greed_index` seems to miss one data point. We will see fix that in the next sections.

We have different dataset that we will use for the predictions but we are still missing one important data used by traders, technical analysis indicator.

Based on a previous research and blog post I wrote, I decided to include a few indicators that are very common in trading:

- Moving average convergence divergence (MACD)

- Rate of change (ROC)

- Bolinger Bands (BB)

- Relative Strenght Index (RSI)

TODO: add https://medium.com/learning-lab/become-a-better-crypto-trader-with-technical-and-chart-analysis-1496b2fc6b85

Before preparing the dataset let's see what would be the features we can extract from the OHLC candlestick data.

## Features

Our candlesticks dataset from coinbase gives us values that are based on the price of bitcoin, but used itself for a machine learning algorithm it will be hard to use those raw absolute values since the price always fluctuate, so we have to think about what defines the candlesticks we have seen above?

If we isolate a candle we can see the following features:

- Size of the body

- Size of the upper shadow / wicks

- Size of the lower shadow / wicks

- Direction of the candle (up or down)

- Closing price

We are now ready to prepare the dataset for the study.

## Preparation

```{r dataset_preparation, cache=TRUE}

enhance_dataset <- function(candles_data, fear_and_greed_index_data, hash_rate_data, average_block_size_data, n_transactions_data, utxo_count_data) {
  candles_enhanced <- candles_data %>%
    mutate(date_only = as.Date(time)) %>%
    left_join(fear_and_greed_index_data, by = c("date_only" = "timestamp")) %>%
    left_join(hash_rate_data, by = c("date_only" = "timestamp")) %>%
    left_join(average_block_size_data, by = c("date_only" = "timestamp")) %>%
    left_join(n_transactions_data, by = c("date_only" = "timestamp")) %>%
    left_join(utxo_count_data, by = c("date_only" = "timestamp")) %>%
    mutate(
      body_size = abs(close - open),
      upper_shadow_size = high - pmax(close, open),
      lower_shadow_size = pmin(close, open) - low,
      direction = ifelse(close > open, "up", "down"),
    ) %>%
    tq_mutate(
      select = close,
      mutate_fun = ROC,
      n = 14,
      col_rename = "roc"
    ) %>%
    tq_mutate( # https://www.keenbase-trading.com/find-best-macd-settings/#t-1719588154943
      select = close,
      mutate_fun = MACD,
      nFast = 12,
      nSlow = 26,
      nSig = 9,
      col_rename = c("macd", "signal")
    ) %>%
    tq_mutate(
      select = close,
      mutate_fun = RSI,
      n = 14,
      col_rename = "rsi"
    ) %>%
    tq_mutate(
      select = close,
      mutate_fun = BBands,
      n = 20,
      sd = 2,
      col_rename = "bband"
    )

  candles_enhanced
}

candles_enhanced <- enhance_dataset(candles, fear_and_greed_index, hash_rate, average_block_size, n_transactions, utxo_count)
knitr::kable(head(candles_enhanced), format = "simple", caption = "Overview of the candlestick dataset enhanced")
```

We have now a table with all the features discussed above. Before adding the lagged features let's explore our set.

First let's see if we have NAs.

```{r dataset_preparation_find_na, cache=TRUE}
na_indexes <- which(apply(candles_enhanced, 1, function(x) any(is.na(x))))
knitr::kable(candles_enhanced[na_indexes, ], format = "simple", caption = "NAs of the dataset")
```

We can see in the table above that there are 2 types of NAs:

1. Technical analisis indicators

2. Fear and greed index

The technical analysis indicators are located at the start of the table which is normal since to calculate these indicators you need to have a certain number of previous values. Clearing those NAs will be enough.

As concern the fear and greed index missing value we can notice that we are missing the values of the day `2024-10-26` so we will add the values of the average between the day before and after manually.

It's important to do so to have coherant lagged values.

```{r fear_and_greed_index_corrected, cache=TRUE}
date_na <- as.Date("2024-10-26")
fear_and_greed_index_date_before_na <- fear_and_greed_index %>% filter(timestamp == as.Date("2024-10-25"))
fear_and_greed_index_date_after_na <- fear_and_greed_index %>% filter(timestamp == as.Date("2024-10-27"))
fear_and_greed_value_date_na <- mean(c(fear_and_greed_index_date_before_na$value, fear_and_greed_index_date_after_na$value))

fear_and_greed_index_corrected <- fear_and_greed_index %>%
  bind_rows(tibble(timestamp = date_na, value = fear_and_greed_value_date_na, value_classification = "Greed"))

fear_and_greed_index %>% filter(timestamp == date_na) %>% nrow()
fear_and_greed_index_corrected %>% filter(timestamp == date_na) %>% nrow()
```

Let's check the NAs again.

```{r dataset_preparation_find_na_corrected, cache=TRUE}
candles_enhanced_cleaned <- enhance_dataset(candles, fear_and_greed_index_corrected, hash_rate, average_block_size, n_transactions, utxo_count)
na_indexes <- which(apply(candles_enhanced_cleaned, 1, function(x) any(is.na(x))))
knitr::kable(candles_enhanced_cleaned[na_indexes, ], format = "simple", caption = "NAs of the dataset cleaned")
```

We can see now the only NAs are the missing TA values that we can clean with a simple line of code.

```{r candles_enhanced_cleaned_no_na, cache=TRUE}
candles_enhanced_cleaned_no_na <- candles_enhanced_cleaned %>% drop_na()
sum(is.na(candles_enhanced_cleaned_no_na))
```


## Visual analysis

First of all let's plot the data to visually verify the data.

TODO Fix rendering of this data (it was fixed previously, could be just a cache issue)

```r
p1 <- candles_enhanced_cleaned_no_na %>%
  ggplot(aes(x = time, y = close)) +
  geom_line(color = "blue") +
  theme_minimal() +
  labs(title = "BTC-USD Price", y = "Price") +
  scale_y_continuous(labels = scales::comma)

p2 <- candles_enhanced_cleaned_no_na %>%
  ggplot(aes(x = time, y = hash_rate)) +
  geom_line(color = "red") +
  theme_minimal() +
  labs(title = "Hash Rate", y = "Hash Rate") +
  scale_y_continuous(labels = scales::comma)

p3 <- candles_enhanced_cleaned_no_na %>%
  ggplot(aes(x = time, y = avg_block_size)) +
  geom_line(color = "green4") +
  theme_minimal() +
  labs(title = "Average Block Size", y = "Size") +
  scale_y_continuous(labels = scales::comma)

p4 <- candles_enhanced_cleaned_no_na %>%
  ggplot(aes(x = time, y = n_transactions)) +
  geom_line(color = "purple") +
  theme_minimal() +
  labs(title = "Number of Transactions", y = "Count") +
  scale_y_continuous(labels = scales::comma)

p5 <- candles_enhanced_cleaned_no_na %>%
  ggplot(aes(x = time, y = utxo_count)) +
  geom_line(color = "orange") +
  theme_minimal() +
  labs(title = "UTXO Count", y = "Count") +
  scale_y_continuous(labels = scales::comma)

p6 <- candles_enhanced_cleaned_no_na %>%
  ggplot(aes(x = time, y = value)) +
  geom_line() +
  theme_minimal() +
  labs(
    title = "BTC-USD Fear and Greed Index Evolution",
    x = "Time",
    y = "Fear and Greed Index"
  ) +
  scale_y_continuous(labels = scales::comma)

# For more readability we are only plotting the last 100 candles
p7 <- candles_enhanced_cleaned_no_na %>%
  tail(24) %>%
  ggplot(aes(x = time, y = volume)) +
  geom_segment(aes(xend = time, yend = 0, color = volume)) +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "BTC-USD Volume Chart (Last 24 candles)", y = "Volume", x = "") +
  theme_tq() +
  theme(legend.position = "none")

combined_plot <- (p1 / p2 / p3 / p4 / p5 / p6 / p7) +
  plot_layout(ncol = 2, heights = c(1, 1, 1, 1)) +
  plot_annotation(
    title = "Bitcoin Price and Blockchain Metrics",
    theme = theme(plot.title = element_text(hjust = 0.5, size = 16))
  ) &
  theme(axis.title.x = element_blank())

combined_plot
```

Find below the candletick chart of BTC-USD.

```r
# For more readability we are only plotting the last 24 candles
p7 <- candles_enhanced_cleaned_no_na %>%
  tail(24) %>%
  mutate(direction = ifelse(close >= open, "up", "down")) %>%
  ggplot(aes(x = time, y = close)) +
  # The shadows (wicks)
  geom_segment(aes(xend = time, y = low, yend = high, color = direction), size = 0.5) +
  # The body
  geom_segment(aes(xend = time, y = open, yend = close, color = direction), size = 5) +
  scale_color_manual(values = c("up" = "darkgreen", "down" = "red")) +
  theme_tq() +
  theme(legend.position = "none") +
  labs(
    title = "BTC-USD Candlestick Chart (Last 24 Candles)",
    x = "Time",
    y = "Price"
  ) +
  scale_y_continuous(labels = scales::comma)
```

And the plot of the different TA.

TODO fix the following for rendering on pdf

```r 
plot_data_ta <- candles_enhanced_cleaned_no_na %>% tail(100)

# ROC Plot
p_roc <- plot_data_ta %>%
  ggplot(aes(x = time, y = roc)) +
  geom_line() +
  labs(title = "Rate of Change (ROC)", y = "ROC") +
  theme_tq() +
  theme(axis.title.x = element_blank())

# Bollinger Bands Plot
p_bbands <- plot_data_ta %>%
  ggplot(aes(x = time, y = close)) +
  geom_line(aes(y = close), color = "blue") + # Close price
  geom_ribbon(aes(ymin = dn, ymax = up), fill = "grey", alpha = 0.4) + # Bollinger Bands area
  geom_line(aes(y = mavg), color = "red", linetype = "dashed") + # Moving Average
  labs(title = "Bollinger Bands (BBands)", y = "Price") +
  theme_tq() +
  theme(axis.title.x = element_blank()) +
  scale_y_continuous(labels = scales::comma)

# MACD Plot
p_macd <- plot_data_ta %>%
  ggplot(aes(x = time)) +
  geom_line(aes(y = macd), color = "blue") +  # MACD line
  geom_line(aes(y = signal), color = "red", linetype = "dashed") + # Signal line
  geom_col(aes(y = macd - signal), alpha = 0.5) + # Histogram of MACD - Signal
  labs(title = "MACD", y = "Value") +
  theme_tq() +
  theme(axis.title.x = element_blank())

# RSI Plot
p_rsi <- plot_data_ta %>%
  ggplot(aes(x = time, y = rsi)) +
  geom_line() +
  geom_hline(yintercept = 70, linetype = "dashed", color = "red") +  # Overbought level
  geom_hline(yintercept = 30, linetype = "dashed", color = "darkgreen") + # Oversold level
  labs(title = "Relative Strength Index (RSI)", y = "RSI") +
  theme_tq() +
  theme(axis.title.x = element_blank())

# Combine TA plots
combined_ta_plot <- (p_roc / p_bbands) | (p_macd / p_rsi)

combined_ta_plot + plot_annotation(
  title = "Technical Analysis Indicators (Last 100 Candles)",
  theme = theme(plot.title = element_text(hjust = 0.5, size = 16))
)

```
Comparing with the data from TradingView it seems that all the charts are correct.

Let's now see how is the distribution of "up" and "down" candles.

```{r visual_repartion, cache=TRUE}
candles_enhanced_cleaned_no_na %>%
  mutate(direction = ifelse(close >= open, "up", "down")) %>%
  summarise(
    up = sum(direction == "up"),
    down = sum(direction == "down")
  ) %>%
  pivot_longer(cols = everything(), names_to = "direction", values_to = "count") %>%
  ggplot(aes(x = direction, y = count, fill = direction)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("up" = "darkgreen", "down" = "red")) +
  theme_minimal() +
  labs(
    title = "Number of Up vs Down Candles",
    x = "Direction",
    y = "Count"
  )
```


```{r distribution_data, cache=TRUE}

distribution_data <- candles_enhanced_cleaned_no_na %>%
  mutate(direction = ifelse(close >= open, "up", "down")) %>%
  summarise(
    up = sum(direction == "up"),
    down = sum(direction == "down")
  ) %>%
  mutate(
    total = up + down,
    up_percentage = up / total,
    down_percentage = down / total
  )

knitr::kable(distribution_data, format = "simple", caption = "Distribution of up and down candles")
```

We can notice that the distribution is not exacly 50%.

## Adding lagged candles

Our study aim at predicting the direction of a candle using the previous candle's data and other features.

So we need to create a function to create a dataset containing lagged candles. We also created another function to directly prepare the right data.

```{r lagged_candles, cache=TRUE}
add_lagged_candles <- function(enhanced_clean_dataset, n_lag) {
  dataset_with_lagged_candles <- enhanced_clean_dataset

  for (i in 1:n_lag) {
    dataset_with_lagged_candles[[paste0("body_size_lag_", i)]] <- lag(dataset_with_lagged_candles$body_size, i)
    dataset_with_lagged_candles[[paste0("upper_shadow_size_lag_", i)]] <- lag(dataset_with_lagged_candles$upper_shadow_size, i)
    dataset_with_lagged_candles[[paste0("lower_shadow_size_lag_", i)]] <- lag(dataset_with_lagged_candles$lower_shadow_size, i)
    dataset_with_lagged_candles[[paste0("direction_lag_", i)]] <- lag(dataset_with_lagged_candles$direction, i)
    dataset_with_lagged_candles[[paste0("volume_lag_", i)]] <- lag(dataset_with_lagged_candles$volume, i)
    dataset_with_lagged_candles[[paste0("value_lag_", i)]] <- lag(dataset_with_lagged_candles$value, i)
    dataset_with_lagged_candles[[paste0("close_lag_", i)]] <- lag(dataset_with_lagged_candles$close, i)
    dataset_with_lagged_candles[[paste0("hash_rate_lag_", i)]] <- lag(dataset_with_lagged_candles$hash_rate, i)
    dataset_with_lagged_candles[[paste0("avg_block_size_lag_", i)]] <- lag(dataset_with_lagged_candles$avg_block_size, i)
    dataset_with_lagged_candles[[paste0("n_transactions_lag_", i)]] <- lag(dataset_with_lagged_candles$n_transactions, i)
    dataset_with_lagged_candles[[paste0("utxo_count_lag_", i)]] <- lag(dataset_with_lagged_candles$utxo_count, i)
    dataset_with_lagged_candles[[paste0("open_lag_", i)]] <- lag(dataset_with_lagged_candles$open, i)
    dataset_with_lagged_candles[[paste0("high_lag_", i)]] <- lag(dataset_with_lagged_candles$high, i)
    dataset_with_lagged_candles[[paste0("low_lag_", i)]] <- lag(dataset_with_lagged_candles$low, i)
    dataset_with_lagged_candles[[paste0("roc_lag_", i)]] <- lag(dataset_with_lagged_candles$roc, i)
    dataset_with_lagged_candles[[paste0("macd_lag_", i)]] <- lag(dataset_with_lagged_candles$macd, i)
    dataset_with_lagged_candles[[paste0("signal_lag_", i)]] <- lag(dataset_with_lagged_candles$signal, i)
    dataset_with_lagged_candles[[paste0("rsi_lag_", i)]] <- lag(dataset_with_lagged_candles$rsi, i)
    dataset_with_lagged_candles[[paste0("up_bband_lag_", i)]] <- lag(dataset_with_lagged_candles$up, i)
    dataset_with_lagged_candles[[paste0("mavg_lag_", i)]] <- lag(dataset_with_lagged_candles$mavg, i)
    dataset_with_lagged_candles[[paste0("dn_bband_lag_", i)]] <- lag(dataset_with_lagged_candles$dn, i)
    dataset_with_lagged_candles[[paste0("pctB_lag_", i)]] <- lag(dataset_with_lagged_candles$pctB, i)
  }

  dataset_with_lagged_candles
}

prepare_dataset <- function(candles_data, fear_and_greed_index_data, hash_rate_data, average_block_size_data, n_transactions_data, utxo_count_data) {
  enhanced_clean_dataset <- enhance_dataset(candles_data, fear_and_greed_index_data, hash_rate_data, average_block_size_data, n_transactions_data, utxo_count_data)
  enhanced_clean_dataset_without_na <- enhanced_clean_dataset %>% drop_na()
  dataset_with_lagged_candles <- add_lagged_candles(enhanced_clean_dataset_without_na, 15)
  dataset_with_lagged_candles_without_na <- dataset_with_lagged_candles %>% drop_na()
  dataset_with_lagged_candles_without_na
}
```

Using the function `prepare_dataset` and the we can have directly the final dataset with lagged data.

## Test and training datasets

We put together the code to fix the fear_and_greed_index and to prepare the datasets and split them in train and test sets.

```{r project_datasets, cache=TRUE}
date_na <- as.Date("2024-10-26")
fear_and_greed_index_date_before_na <- fear_and_greed_index %>% filter(timestamp == as.Date("2024-10-25"))
fear_and_greed_index_date_after_na <- fear_and_greed_index %>% filter(timestamp == as.Date("2024-10-27"))
fear_and_greed_value_date_na <- mean(c(fear_and_greed_index_date_before_na$value, fear_and_greed_index_date_after_na$value))

fear_and_greed_index_corrected <- fear_and_greed_index %>%
  bind_rows(tibble(timestamp = date_na, value = fear_and_greed_value_date_na, value_classification = "Greed"))

project_dataset <- prepare_dataset(candles, fear_and_greed_index_corrected, hash_rate, average_block_size, n_transactions, utxo_count)

sum(is.na(project_dataset))
nrow(project_dataset)
nrow(candles)

test_index <- createDataPartition(y = project_dataset$direction, times = 1, p = 0.2, list = FALSE)
train_set <- project_dataset[-test_index, ]
test_set <- project_dataset[test_index, ]
```
The number of rows reduced since adding lags introduced a lot of NAs in the first rows, NAs that we removed. Also we decided not to use cross validation to reduce the time of training for the different machine learnings algorithms. Note that we initiated already the project using `set.seed(1)` part of the global variables.

## Machine learning algorithms

Based on some shallow research on the press, we selected the following machine learnings algorithms that seems to work better with our type of dataset:

- Generalized Linear Model (GLM)

- Decision Tree (DT)

- Random Forest (RF)

- K-nearest neighbor (KNN)

- Gradient boosting (GBM)

TODO add links reference
https://www.neuroquantology.com/open-access/An+Optimized+Machine+Learning+Model+for+Candlestick+Chart+Analysis+to+Predict+Stock+Market+Trends_9861/?download=true
https://arxiv.org/pdf/1606.00930

We will also compare these algorithms with Random guess as a reference.

## Utility functions

Since we want to compare each algorithms for a different set of features we need a function to create the formula that we will pass to the machine learning function.

```{r utility_functions, cache=TRUE}
create_feature_formula <- function(feature_names, n_lags) {
  features <- c()

  for (feature_name in feature_names) {
    for (i in 1:n_lags) {
      features <- c(
        features,
        paste0(feature_name, "_lag_", i)
      )
    }
  }

  formula_str <- paste("direction ~", paste(features, collapse = " + "))

  as.formula(formula_str)
}


train_with_cache <- function(formula, train_set, method) {
  formula_hash <- digest::digest(formula)
  filepath <- paste0("models/", method, "_", formula_hash, ".rds")
  if (file.exists(filepath)) {
    model <- readRDS(filepath)
    print(paste("Model loaded from cache:", filepath))
  } else {
    start_time <- Sys.time()
    if (method == "rf") {
      model <- train(formula, data = train_set, method = "rf", ntree = 100)
    } else if (method == "glm") {
      model <- train(formula, data = train_set, method = "glm", family = "binomial")
    } else if (method == "rpart") {
      model <- train(formula, data = train_set, method = "rpart")
    } else if (method == "knn") {
      model <- train(formula, data = train_set, method = "knn", preProcess = c("center", "scale"), tuneGrid = data.frame(k = seq(3, 15, 2)))
    } else if (method == "gbm") {
      model <- train(formula, data = train_set, method = "gbm")
    } else {
      stop("Invalid method")
    }
    end_time <- Sys.time()
    print(paste("Training time:", format(end_time - start_time, digits = 2)))

    saveRDS(model, filepath)
  }

  model
}

evaluate_models <- function(feature_set, test_set, lags = c(1, 3, 5, 7, 15)) {
  # Define model types
  model_types <- c("glm", "rf", "rpart", "knn", "gbm")

  # Create a data frame to store results
  results <- data.frame(
    model = character(),
    model_type = character(),
    lag = numeric(),
    accuracy = numeric(),
    stringsAsFactors = FALSE
  )

  # Evaluate each model type and lag combination
  for (model_type in model_types) {
    for (lag in lags) {
      model_name <- paste0(model_type, "_model_", feature_set, "_lag_", lag)

      if (exists(model_name)) {
        # Get the model object
        model <- get(model_name)

        # Make predictions
        predictions <- predict(model, test_set)

        # Calculate accuracy
        accuracy <- mean(predictions == test_set$direction)

        # Add to results
        results <- rbind(results, data.frame(
          model = model_name,
          model_type = model_type,
          lag = lag,
          accuracy = accuracy,
          stringsAsFactors = FALSE
        ))
      }
    }
  }

  # Sort by accuracy in descending order
  results <- results[order(-results$accuracy), ]

  # Add rank column
  results$rank <- 1:nrow(results)

  results
}
```

# Training machine learnings algorithms

We are now going to train and compare various machine learning algorithms with different set of selected features and different number of lag.

Before starting with those algorithms we will start with very simple ones to have a reference to compare.

## Simple algorithms

### Random guess

We will run a montecarlo simulation of `1000` random guesses of direction and compare it with the test set.

```{r random_guess, cache=TRUE}
random_guess_simulations <- replicate(1000, {
  estimated_direction <- replicate(nrow(test_set), sample(c("up", "down"), 1))
  mean(estimated_direction == test_set$direction)
})

mean_accuracy <- mean(random_guess_simulations)
print(paste("Random guess simulation results (10000 runs):"))
print(paste("Mean accuracy:", round(mean_accuracy, 4)))
```


### Always up

We can also compare this with an always up strategy:
```{r always_up, cache=TRUE}
# Return always "up"
always_up <- function(test_set) {
  replicate(nrow(test_set), "up")
}
always_up_accuracy <- mean(always_up(test_set) == test_set$direction)
print(paste("Always up accuracy:", round(always_up_accuracy, 4)))
```

### Previous direction

Now let's see how it compares with returning the same direction as the previous (lag_1):
```{r previous_direction, cache=TRUE}
previous_direction <- function(test_set) {
  test_set$direction_lag_1
}
previous_direction_accuracy <- mean(previous_direction(test_set) == test_set$direction)
print(paste("Previous direction accuracy:", round(previous_direction_accuracy, 4)))
```

### Opposite direction to previous one
```{r opposite_direction_previous, cache=TRUE}
opposite_direction <- function(test_set) {
  ifelse(test_set$direction_lag_1 == "up", "down", "up")
}
opposite_direction_accuracy <- mean(opposite_direction(test_set) == test_set$direction)
print(paste("Opposite direction to the previous one accuracy:", round(opposite_direction_accuracy, 4)))
```

We can see that giving the opposite direction to the previous one has the highest accuracy so far: `0.5342`, let's see how it compares with machine learning algorithms.

## Machine learning algorithms

For each of these machine learning algorithms we will use either 1, 3, 5, 7 or 12 lagged data.
We will later fine tune that number of lagged candles to see what works the best.

### OHLC features

We will first try to use the lagged OHLC features got directly from the coinbase dataset:

- open

- high

- low

- close

- volume





```{r ohlc_features, cache=TRUE}
formula_OHLC_lag_1 <- create_feature_formula(c("open", "high", "low", "close", "volume"), 1)
formula_OHLC_lag_3 <- create_feature_formula(c("open", "high", "low", "close", "volume"), 3)
formula_OHLC_lag_5 <- create_feature_formula(c("open", "high", "low", "close", "volume"), 5)
formula_OHLC_lag_7 <- create_feature_formula(c("open", "high", "low", "close", "volume"), 7)
formula_OHLC_lag_15 <- create_feature_formula(c("open", "high", "low", "close", "volume"), 15)

glm_model_OHLC_lag_1 <- train_with_cache(formula_OHLC_lag_1, train_set, "glm")
glm_model_OHLC_lag_3 <- train_with_cache(formula_OHLC_lag_3, train_set, "glm")
glm_model_OHLC_lag_5 <- train_with_cache(formula_OHLC_lag_5, train_set, "glm")
glm_model_OHLC_lag_7 <- train_with_cache(formula_OHLC_lag_7, train_set, "glm")
glm_model_OHLC_lag_15 <- train_with_cache(formula_OHLC_lag_15, train_set, "glm")

rpart_model_OHLC_lag_1 <- train_with_cache(formula_OHLC_lag_1, train_set, "rpart")
rpart_model_OHLC_lag_3 <- train_with_cache(formula_OHLC_lag_3, train_set, "rpart")
rpart_model_OHLC_lag_5 <- train_with_cache(formula_OHLC_lag_5, train_set, "rpart")
rpart_model_OHLC_lag_7 <- train_with_cache(formula_OHLC_lag_7, train_set, "rpart")
rpart_model_OHLC_lag_15 <- train_with_cache(formula_OHLC_lag_15, train_set, "rpart")

rf_model_OHLC_lag_1 <- train_with_cache(formula_OHLC_lag_1, train_set, "rf")
rf_model_OHLC_lag_3 <- train_with_cache(formula_OHLC_lag_3, train_set, "rf")
rf_model_OHLC_lag_5 <- train_with_cache(formula_OHLC_lag_5, train_set, "rf")
rf_model_OHLC_lag_7 <- train_with_cache(formula_OHLC_lag_7, train_set, "rf")
rf_model_OHLC_lag_15 <- train_with_cache(formula_OHLC_lag_15, train_set, "rf")

knn_model_OHLC_lag_1 <- train_with_cache(formula_OHLC_lag_1, train_set, "knn")
knn_model_OHLC_lag_3 <- train_with_cache(formula_OHLC_lag_3, train_set, "knn")
knn_model_OHLC_lag_5 <- train_with_cache(formula_OHLC_lag_5, train_set, "knn")
knn_model_OHLC_lag_7 <- train_with_cache(formula_OHLC_lag_7, train_set, "knn")
knn_model_OHLC_lag_15 <- train_with_cache(formula_OHLC_lag_15, train_set, "knn")

gbm_model_OHLC_lag_1 <- train_with_cache(formula_OHLC_lag_1, train_set, "gbm")
gbm_model_OHLC_lag_3 <- train_with_cache(formula_OHLC_lag_3, train_set, "gbm")
gbm_model_OHLC_lag_5 <- train_with_cache(formula_OHLC_lag_5, train_set, "gbm")
gbm_model_OHLC_lag_7 <- train_with_cache(formula_OHLC_lag_7, train_set, "gbm")
gbm_model_OHLC_lag_15 <- train_with_cache(formula_OHLC_lag_15, train_set, "gbm")

results_OHLC <- evaluate_models("OHLC", test_set)
knitr::kable(results_OHLC, format = "simple", caption = "Model comparison for OHLC features")
```

```{r ohlc_stats, cache=TRUE}
summary_stats_OHLC <- aggregate(accuracy ~ model_type,
  data = results_OHLC,
  FUN = function(x) c(mean = mean(x), sd = sd(x), max = max(x))
)

summary_stats_OHLC <- data.frame(
  model_type = summary_stats_OHLC$model_type,
  mean_accuracy = summary_stats_OHLC$accuracy[,"mean"],
  sd_accuracy = summary_stats_OHLC$accuracy[,"sd"],
  max_accuracy = summary_stats_OHLC$accuracy[,"max"]
)

knitr::kable(summary_stats_OHLC, 
             format = "simple", 
             caption = "Summary statistics for OHLC features",
             digits = 4,
             col.names = c("Model Type", "Mean Accuracy", "SD Accuracy", "Max Accuracy"))
```

### Candle features

Now let's try to use the lagged candle features:

- body_size

- upper_shadow_size

- lower_shadow_size

- direction

- close

- volume



```{r candles_features, cache=TRUE}
formula_candles_lag_1 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "volume"), 1)
formula_candles_lag_3 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "volume"), 3)
formula_candles_lag_5 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "volume"), 5)
formula_candles_lag_7 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "volume"), 7)
formula_candles_lag_15 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "volume"), 15)

glm_model_candles_lag_1 <- train_with_cache(formula_candles_lag_1, train_set, "glm")
glm_model_candles_lag_3 <- train_with_cache(formula_candles_lag_3, train_set, "glm")
glm_model_candles_lag_5 <- train_with_cache(formula_candles_lag_5, train_set, "glm")
glm_model_candles_lag_7 <- train_with_cache(formula_candles_lag_7, train_set, "glm")
glm_model_candles_lag_15 <- train_with_cache(formula_candles_lag_15, train_set, "glm")

rpart_model_candles_lag_1 <- train_with_cache(formula_candles_lag_1, train_set, "rpart")
rpart_model_candles_lag_3 <- train_with_cache(formula_candles_lag_3, train_set, "rpart")
rpart_model_candles_lag_5 <- train_with_cache(formula_candles_lag_5, train_set, "rpart")
rpart_model_candles_lag_7 <- train_with_cache(formula_candles_lag_7, train_set, "rpart")
rpart_model_candles_lag_15 <- train_with_cache(formula_candles_lag_15, train_set, "rpart")

rf_model_candles_lag_1 <- train_with_cache(formula_candles_lag_1, train_set, "rf")
rf_model_candles_lag_3 <- train_with_cache(formula_candles_lag_3, train_set, "rf")
rf_model_candles_lag_5 <- train_with_cache(formula_candles_lag_5, train_set, "rf")
rf_model_candles_lag_7 <- train_with_cache(formula_candles_lag_7, train_set, "rf")
rf_model_candles_lag_15 <- train_with_cache(formula_candles_lag_15, train_set, "rf")

knn_model_candles_lag_1 <- train_with_cache(formula_candles_lag_1, train_set, "knn")
knn_model_candles_lag_3 <- train_with_cache(formula_candles_lag_3, train_set, "knn")
knn_model_candles_lag_5 <- train_with_cache(formula_candles_lag_5, train_set, "knn")
knn_model_candles_lag_7 <- train_with_cache(formula_candles_lag_7, train_set, "knn")
knn_model_candles_lag_15 <- train_with_cache(formula_candles_lag_15, train_set, "knn")

gbm_model_candles_lag_1 <- train_with_cache(formula_candles_lag_1, train_set, "gbm")
gbm_model_candles_lag_3 <- train_with_cache(formula_candles_lag_3, train_set, "gbm")
gbm_model_candles_lag_5 <- train_with_cache(formula_candles_lag_5, train_set, "gbm")
gbm_model_candles_lag_7 <- train_with_cache(formula_candles_lag_7, train_set, "gbm")
gbm_model_candles_lag_15 <- train_with_cache(formula_candles_lag_15, train_set, "gbm")

results_candles <- evaluate_models("candles", test_set)
results_candles
```

```{r candles_stats, cache=TRUE}
summary_stats_candles <- aggregate(accuracy ~ model_type,
  data = results_candles,
  FUN = function(x) c(mean = mean(x), sd = sd(x), max = max(x))
)

summary_stats_candles <- data.frame(
  model_type = summary_stats_candles$model_type,
  mean_accuracy = summary_stats_candles$accuracy[,"mean"],
  sd_accuracy = summary_stats_candles$accuracy[,"sd"],
  max_accuracy = summary_stats_candles$accuracy[,"max"]
)

knitr::kable(summary_stats_candles, 
             format = "simple", 
             caption = "Summary statistics for candles features",
             digits = 4,
             col.names = c("Model Type", "Mean Accuracy", "SD Accuracy", "Max Accuracy"))
```

### Candles features and fear and greed index

Now let's try to use the lagged candles features and the fear and greed index:

- body_size

- upper_shadow_size

- lower_shadow_size

- direction

- close

- value

- volume

```{r candles_fg_features, cache=TRUE}
formula_candles_fg_lag_1 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "volume"), 1)
formula_candles_fg_lag_3 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "volume"), 3)
formula_candles_fg_lag_5 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "volume"), 5)
formula_candles_fg_lag_7 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "volume"), 7)
formula_candles_fg_lag_15 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "volume"), 15)

glm_model_candles_fg_lag_1 <- train_with_cache(formula_candles_fg_lag_1, train_set, "glm")
glm_model_candles_fg_lag_3 <- train_with_cache(formula_candles_fg_lag_3, train_set, "glm")
glm_model_candles_fg_lag_5 <- train_with_cache(formula_candles_fg_lag_5, train_set, "glm")
glm_model_candles_fg_lag_7 <- train_with_cache(formula_candles_fg_lag_7, train_set, "glm")
glm_model_candles_fg_lag_15 <- train_with_cache(formula_candles_fg_lag_15, train_set, "glm")

rpart_model_candles_fg_lag_1 <- train_with_cache(formula_candles_fg_lag_1, train_set, "rpart")
rpart_model_candles_fg_lag_3 <- train_with_cache(formula_candles_fg_lag_3, train_set, "rpart")
rpart_model_candles_fg_lag_5 <- train_with_cache(formula_candles_fg_lag_5, train_set, "rpart")
rpart_model_candles_fg_lag_7 <- train_with_cache(formula_candles_fg_lag_7, train_set, "rpart")
rpart_model_candles_fg_lag_15 <- train_with_cache(formula_candles_fg_lag_15, train_set, "rpart")

rf_model_candles_fg_lag_1 <- train_with_cache(formula_candles_fg_lag_1, train_set, "rf")
rf_model_candles_fg_lag_3 <- train_with_cache(formula_candles_fg_lag_3, train_set, "rf")
rf_model_candles_fg_lag_5 <- train_with_cache(formula_candles_fg_lag_5, train_set, "rf")
rf_model_candles_fg_lag_7 <- train_with_cache(formula_candles_fg_lag_7, train_set, "rf")
rf_model_candles_fg_lag_15 <- train_with_cache(formula_candles_fg_lag_15, train_set, "rf")

knn_model_candles_fg_lag_1 <- train_with_cache(formula_candles_fg_lag_1, train_set, "knn")
knn_model_candles_fg_lag_3 <- train_with_cache(formula_candles_fg_lag_3, train_set, "knn")
knn_model_candles_fg_lag_5 <- train_with_cache(formula_candles_fg_lag_5, train_set, "knn")
knn_model_candles_fg_lag_7 <- train_with_cache(formula_candles_fg_lag_7, train_set, "knn")
knn_model_candles_fg_lag_15 <- train_with_cache(formula_candles_fg_lag_15, train_set, "knn")

gbm_model_candles_fg_lag_1 <- train_with_cache(formula_candles_fg_lag_1, train_set, "gbm")
gbm_model_candles_fg_lag_3 <- train_with_cache(formula_candles_fg_lag_3, train_set, "gbm")
gbm_model_candles_fg_lag_5 <- train_with_cache(formula_candles_fg_lag_5, train_set, "gbm")
gbm_model_candles_fg_lag_7 <- train_with_cache(formula_candles_fg_lag_7, train_set, "gbm")
gbm_model_candles_fg_lag_15 <- train_with_cache(formula_candles_fg_lag_15, train_set, "gbm")

results_candles_fg <- evaluate_models("candles_fg", test_set)
results_candles_fg
```

```{r candles_fg_stats, cache=TRUE}
summary_stats_candles_fg <- aggregate(accuracy ~ model_type,
  data = results_candles_fg,
  FUN = function(x) c(mean = mean(x), sd = sd(x), max = max(x))
)

summary_stats_candles_fg <- data.frame(
  model_type = summary_stats_candles_fg$model_type,
  mean_accuracy = summary_stats_candles_fg$accuracy[,"mean"],
  sd_accuracy = summary_stats_candles_fg$accuracy[,"sd"],
  max_accuracy = summary_stats_candles_fg$accuracy[,"max"]
)

knitr::kable(summary_stats_candles_fg, 
             format = "simple", 
             caption = "Summary statistics for candles features and fear and greed index",
             digits = 4,
             col.names = c("Model Type", "Mean Accuracy", "SD Accuracy", "Max Accuracy"))
```

### Candles features, fear and greed index and chain data

We will try to use the lagged candles features, the fear and greed index and the chain data:

- body_size

- upper_shadow_size

- lower_shadow_size

- direction

- close

- value

- hash_rate

- avg_block_size

- n_transactions

- utxo_count

- volume



```{r candles_fg_chain_features, cache=TRUE}
formula_candles_fg_chain_lag_1 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "volume"), 1)
formula_candles_fg_chain_lag_3 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "volume"), 3)
formula_candles_fg_chain_lag_5 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "volume"), 5)
formula_candles_fg_chain_lag_7 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "volume"), 7)
formula_candles_fg_chain_lag_15 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "volume"), 15)

glm_model_candles_fg_chain_lag_1 <- train_with_cache(formula_candles_fg_chain_lag_1, train_set, "glm")
glm_model_candles_fg_chain_lag_3 <- train_with_cache(formula_candles_fg_chain_lag_3, train_set, "glm")
glm_model_candles_fg_chain_lag_5 <- train_with_cache(formula_candles_fg_chain_lag_5, train_set, "glm")
glm_model_candles_fg_chain_lag_7 <- train_with_cache(formula_candles_fg_chain_lag_7, train_set, "glm")
glm_model_candles_fg_chain_lag_15 <- train_with_cache(formula_candles_fg_chain_lag_15, train_set, "glm")

rpart_model_candles_fg_chain_lag_1 <- train_with_cache(formula_candles_fg_chain_lag_1, train_set, "rpart")
rpart_model_candles_fg_chain_lag_3 <- train_with_cache(formula_candles_fg_chain_lag_3, train_set, "rpart")
rpart_model_candles_fg_chain_lag_5 <- train_with_cache(formula_candles_fg_chain_lag_5, train_set, "rpart")
rpart_model_candles_fg_chain_lag_7 <- train_with_cache(formula_candles_fg_chain_lag_7, train_set, "rpart")
rpart_model_candles_fg_chain_lag_15 <- train_with_cache(formula_candles_fg_chain_lag_15, train_set, "rpart")

rf_model_candles_fg_chain_lag_1 <- train_with_cache(formula_candles_fg_chain_lag_1, train_set, "rf")
rf_model_candles_fg_chain_lag_3 <- train_with_cache(formula_candles_fg_chain_lag_3, train_set, "rf")
rf_model_candles_fg_chain_lag_5 <- train_with_cache(formula_candles_fg_chain_lag_5, train_set, "rf")
rf_model_candles_fg_chain_lag_7 <- train_with_cache(formula_candles_fg_chain_lag_7, train_set, "rf")
rf_model_candles_fg_chain_lag_15 <- train_with_cache(formula_candles_fg_chain_lag_15, train_set, "rf")

knn_model_candles_fg_chain_lag_1 <- train_with_cache(formula_candles_fg_chain_lag_1, train_set, "knn")
knn_model_candles_fg_chain_lag_3 <- train_with_cache(formula_candles_fg_chain_lag_3, train_set, "knn")
knn_model_candles_fg_chain_lag_5 <- train_with_cache(formula_candles_fg_chain_lag_5, train_set, "knn")
knn_model_candles_fg_chain_lag_7 <- train_with_cache(formula_candles_fg_chain_lag_7, train_set, "knn")
knn_model_candles_fg_chain_lag_15 <- train_with_cache(formula_candles_fg_chain_lag_15, train_set, "knn")

gbm_model_candles_fg_chain_lag_1 <- train_with_cache(formula_candles_fg_chain_lag_1, train_set, "gbm")
gbm_model_candles_fg_chain_lag_3 <- train_with_cache(formula_candles_fg_chain_lag_3, train_set, "gbm")
gbm_model_candles_fg_chain_lag_5 <- train_with_cache(formula_candles_fg_chain_lag_5, train_set, "gbm")
gbm_model_candles_fg_chain_lag_7 <- train_with_cache(formula_candles_fg_chain_lag_7, train_set, "gbm")
gbm_model_candles_fg_chain_lag_15 <- train_with_cache(formula_candles_fg_chain_lag_15, train_set, "gbm")

results_candles_fg_chain <- evaluate_models("candles_fg_chain", test_set)
results_candles_fg_chain
```

```{r candles_fg_chain_stats, cache=TRUE}
summary_stats_candles_fg_chain <- aggregate(accuracy ~ model_type,
  data = results_candles_fg_chain,
  FUN = function(x) c(mean = mean(x), sd = sd(x), max = max(x))
)

summary_stats_candles_fg_chain <- data.frame(
  model_type = summary_stats_candles_fg_chain$model_type,
  mean_accuracy = summary_stats_candles_fg_chain$accuracy[,"mean"],
  sd_accuracy = summary_stats_candles_fg_chain$accuracy[,"sd"],
  max_accuracy = summary_stats_candles_fg_chain$accuracy[,"max"]
)

knitr::kable(summary_stats_candles_fg_chain, 
             format = "simple", 
             caption = "Summary statistics for candles features, fear and greed index and chain data",
             digits = 4,
             col.names = c("Model Type", "Mean Accuracy", "SD Accuracy", "Max Accuracy"))
```

### Candles features, fear and greed index, chain data and technical analysis indicators

Finally let's add the technical analysis indicators to the model, so we will use the following lagged features:

- body_size

- upper_shadow_size

- lower_shadow_size

- direction

- close

- value

- hash_rate

- avg_block_size

- n_transactions

- utxo_count

- volume

- roc

- macd

- signal

- rsi

- up_bband

- mavg

- dn_bband

- pctB

```{r candles_fg_chain_ta_features, cache=TRUE}
formula_candles_fg_chain_ta_lag_1 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 1)
formula_candles_fg_chain_ta_lag_3 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 3)
formula_candles_fg_chain_ta_lag_5 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 5)
formula_candles_fg_chain_ta_lag_7 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 7)
formula_candles_fg_chain_ta_lag_15 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 15)

glm_model_candles_fg_chain_ta_lag_1 <- train_with_cache(formula_candles_fg_chain_ta_lag_1, train_set, "glm")
glm_model_candles_fg_chain_ta_lag_3 <- train_with_cache(formula_candles_fg_chain_ta_lag_3, train_set, "glm")
glm_model_candles_fg_chain_ta_lag_5 <- train_with_cache(formula_candles_fg_chain_ta_lag_5, train_set, "glm")
glm_model_candles_fg_chain_ta_lag_7 <- train_with_cache(formula_candles_fg_chain_ta_lag_7, train_set, "glm")
glm_model_candles_fg_chain_ta_lag_15 <- train_with_cache(formula_candles_fg_chain_ta_lag_15, train_set, "glm")

rpart_model_candles_fg_chain_ta_lag_1 <- train_with_cache(formula_candles_fg_chain_ta_lag_1, train_set, "rpart")
rpart_model_candles_fg_chain_ta_lag_3 <- train_with_cache(formula_candles_fg_chain_ta_lag_3, train_set, "rpart")
rpart_model_candles_fg_chain_ta_lag_5 <- train_with_cache(formula_candles_fg_chain_ta_lag_5, train_set, "rpart")
rpart_model_candles_fg_chain_ta_lag_7 <- train_with_cache(formula_candles_fg_chain_ta_lag_7, train_set, "rpart")
rpart_model_candles_fg_chain_ta_lag_15 <- train_with_cache(formula_candles_fg_chain_ta_lag_15, train_set, "rpart")

rf_model_candles_fg_chain_ta_lag_1 <- train_with_cache(formula_candles_fg_chain_ta_lag_1, train_set, "rf")
rf_model_candles_fg_chain_ta_lag_3 <- train_with_cache(formula_candles_fg_chain_ta_lag_3, train_set, "rf")
rf_model_candles_fg_chain_ta_lag_5 <- train_with_cache(formula_candles_fg_chain_ta_lag_5, train_set, "rf")
rf_model_candles_fg_chain_ta_lag_7 <- train_with_cache(formula_candles_fg_chain_ta_lag_7, train_set, "rf")
rf_model_candles_fg_chain_ta_lag_15 <- train_with_cache(formula_candles_fg_chain_ta_lag_15, train_set, "rf")

knn_model_candles_fg_chain_ta_lag_1 <- train_with_cache(formula_candles_fg_chain_ta_lag_1, train_set, "knn")
knn_model_candles_fg_chain_ta_lag_3 <- train_with_cache(formula_candles_fg_chain_ta_lag_3, train_set, "knn")
knn_model_candles_fg_chain_ta_lag_5 <- train_with_cache(formula_candles_fg_chain_ta_lag_5, train_set, "knn")
knn_model_candles_fg_chain_ta_lag_7 <- train_with_cache(formula_candles_fg_chain_ta_lag_7, train_set, "knn")
knn_model_candles_fg_chain_ta_lag_15 <- train_with_cache(formula_candles_fg_chain_ta_lag_15, train_set, "knn")

gbm_model_candles_fg_chain_ta_lag_1 <- train_with_cache(formula_candles_fg_chain_ta_lag_1, train_set, "gbm")
gbm_model_candles_fg_chain_ta_lag_3 <- train_with_cache(formula_candles_fg_chain_ta_lag_3, train_set, "gbm")
gbm_model_candles_fg_chain_ta_lag_5 <- train_with_cache(formula_candles_fg_chain_ta_lag_5, train_set, "gbm")
gbm_model_candles_fg_chain_ta_lag_7 <- train_with_cache(formula_candles_fg_chain_ta_lag_7, train_set, "gbm")
gbm_model_candles_fg_chain_ta_lag_15 <- train_with_cache(formula_candles_fg_chain_ta_lag_15, train_set, "gbm")

results_candles_fg_chain_ta <- evaluate_models("candles_fg_chain_ta", test_set)
results_candles_fg_chain_ta
```

```{r candles_fg_chain_ta_stats, cache=TRUE}
summary_stats_candles_fg_chain_ta <- aggregate(accuracy ~ model_type,
  data = results_candles_fg_chain_ta,
  FUN = function(x) c(mean = mean(x), sd = sd(x), max = max(x))
)

summary_stats_candles_fg_chain_ta <- data.frame(
  model_type = summary_stats_candles_fg_chain_ta$model_type,
  mean_accuracy = summary_stats_candles_fg_chain_ta$accuracy[,"mean"],
  sd_accuracy = summary_stats_candles_fg_chain_ta$accuracy[,"sd"],
  max_accuracy = summary_stats_candles_fg_chain_ta$accuracy[,"max"]
)

knitr::kable(summary_stats_candles_fg_chain_ta, 
             format = "simple", 
             caption = "Summary statistics for candles features, fear and greed index, chain data and technical analysis indicators",
             digits = 4,
             col.names = c("Model Type", "Mean Accuracy", "SD Accuracy", "Max Accuracy"))
```

## Models comparison

```{r best_models, cache=TRUE}
feature_sets <- c("OHLC", "candles", "candles_fg", "candles_fg_chain", "candles_fg_chain_ta")

# Function to get top models across all feature sets
get_top_models <- function(test_set, n = 10) {
  all_results <- data.frame()

  for (feature_set in feature_sets) {
    results <- evaluate_models(feature_set, test_set)
    all_results <- rbind(all_results, results)
  }

  # Sort by accuracy and get top n
  all_results <- all_results[order(-all_results$accuracy), ]
  head(all_results, n)
}

get_top_models(test_set)
```

```{r best_features, cache=TRUE}
feature_set_summary <- data.frame()
for (feature_set in feature_sets) {
  results <- evaluate_models(feature_set, test_set)
  avg_accuracy <- mean(results$accuracy)
  sd_accuracy <- sd(results$accuracy)
  feature_set_summary <- rbind(
    feature_set_summary,
    data.frame(
      feature_set = feature_set,
      avg_accuracy = avg_accuracy,
      sd_accuracy = sd_accuracy
    )
  )
}
feature_set_summary <- feature_set_summary[order(-feature_set_summary$avg_accuracy), ]
feature_set_summary
```

As we can see the best models are the ones using `Gradient boosting`  and the `candles` feature set seems to perform better overall.

Also as expected the OHLC didn't perform well since it just uses raw data that are hard to use to train a machine learning algorithm.

But in order to proceed to the fine tuning we will use `gbm_model_candles_fg_chain_ta_lag_1` since it's outperforming the other models with an accuracy of `0.5498615`.

# Fine tuning

Before proceeding to the fine tuning it's worth checking if the GBM model is not performing better with the same features and 2 lags instead of one.

```{r gbm_model_candles_fg_chain_ta_lag_2, cache=TRUE}
formula_candles_fg_chain_ta_lag_2 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 2)
gbm_model_candles_fg_chain_ta_lag_2 <- train_with_cache(formula_candles_fg_chain_ta_lag_2, train_set, "gbm")

accuracy_gbm_model_candles_fg_chain_ta_lag_2 <- mean(predict(gbm_model_candles_fg_chain_ta_lag_2, test_set) == test_set$direction)
accuracy_gbm_model_candles_fg_chain_ta_lag_2
```
As we can see the GBM model with `candles_fg_chain_ta` feature set and 1 lag `gbm_model_candles_fg_chain_ta_lag_1` is still performing better.

Let's use its tuning values and let's fine tune it.

```{r gbm_model_candles_fg_chain_ta_lag_1_bestTune, cache=TRUE}
gbm_model_candles_fg_chain_ta_lag_1$bestTune
```

We will use values around those values to fine tune this algorithm. Also unlike all the others algorithms we will use cross-validation for avoiding overfitting and having a more robust prediction algorithm, that would perform better with any dataset than only the `test_set`.

```{r gbm_model_candles_fg_chain_ta_lag_1_tuned, cache=TRUE}
# Define the tuning grid with the best values
gbm_grid <- expand.grid(
  n.trees = c(45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55),
  interaction.depth = c(1, 2),
  shrinkage = c(0.05, 0.1, 0.15),
  n.minobsinnode = c(8, 9, 10, 11, 12)
)

# Set up cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5, # 5-fold cross-validation
  verboseIter = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Train the fine-tuned model with cross-validation
formula_candles_fg_chain_ta_lag_1 <- create_feature_formula(c("body_size", "upper_shadow_size", "lower_shadow_size", "direction", "close", "value", "hash_rate", "avg_block_size", "n_transactions", "utxo_count", "roc", "macd", "signal", "rsi", "up_bband", "mavg", "dn_bband", "pctB", "volume"), 1)


if (!file.exists("gbm_model_candles_fg_chain_ta_lag_1_tuned.rds")) {
  gbm_model_candles_fg_chain_ta_lag_1_tuned <- train(
    formula_candles_fg_chain_ta_lag_1,
    data = train_set,
    method = "gbm",
    trControl = train_control,
    tuneGrid = gbm_grid,
    metric = "ROC"
  )
  saveRDS(gbm_model_candles_fg_chain_ta_lag_1_tuned, "gbm_model_candles_fg_chain_ta_lag_1_tuned.rds")
} else {
  gbm_model_candles_fg_chain_ta_lag_1_tuned <- readRDS("gbm_model_candles_fg_chain_ta_lag_1_tuned.rds")
}

# Evaluate the fine-tuned model on the test set
accuracy_gbm_model_candles_fg_chain_ta_lag_1_tuned <- mean(predict(gbm_model_candles_fg_chain_ta_lag_1_tuned, test_set) == test_set$direction)
print(paste("Fine-tuned model accuracy:", accuracy_gbm_model_candles_fg_chain_ta_lag_1_tuned))
```
As we can see the result the model is performing slightly less good than the one without fine tuning, but it's still better than the third best model.

We can see below the values of the different parameters:
```{r gbm_model_candles_fg_chain_ta_lag_1_tuned_best_tune, cache=TRUE}
gbm_model_candles_fg_chain_ta_lag_1_tuned$bestTune
```
Now let's compare the results and analyse what we have got.


# Results

We will compare the best model for each feature set.

```{r results_comparison, cache=TRUE}
# Get the best model from each feature set
best_OHLC <- results_OHLC[which.max(results_OHLC$accuracy), ] %>% select(-rank) %>% mutate(features = "OHLC")
best_candles <- results_candles[which.max(results_candles$accuracy), ] %>% select(-rank) %>% mutate(features = "Candles")
best_candles_fg <- results_candles_fg[which.max(results_candles_fg$accuracy), ] %>% select(-rank) %>% mutate(features = "Candles, F&G")
best_candles_fg_chain <- results_candles_fg_chain[which.max(results_candles_fg_chain$accuracy), ] %>% select(-rank) %>% mutate(features = "Candles, F&G, Chain")
best_candles_fg_chain_ta <- results_candles_fg_chain_ta[which.max(results_candles_fg_chain_ta$accuracy), ] %>% select(-rank) %>% mutate(features = "Candles, F&G, Chain, TA")

# Create a data frame for the tuned model
tuned_model <- data.frame(
  model = "gbm_model_candles_fg_chain_ta_lag_1_tuned",
  features = "Candles, F&G, Chain, TA",
  model_type = "gbm",
  lag = 1,
  accuracy = accuracy_gbm_model_candles_fg_chain_ta_lag_1_tuned,
  stringsAsFactors = FALSE
)

# Create a data frame for the baseline methods
simple_models <- data.frame(
  model = c("random_guess", "always_up", "previous_direction", "opposite_direction"),
  features = c("simple", "simple", "simple", "simple"),
  model_type = c("simple", "simple", "simple", "simple"),
  lag = c(NA, NA, 1, 1),
  accuracy = c(mean_accuracy, always_up_accuracy, previous_direction_accuracy, opposite_direction_accuracy),
  stringsAsFactors = FALSE
)

# Combine all results
all_best_models <- rbind(
  simple_models,
  best_OHLC,
  best_candles,
  best_candles_fg,
  best_candles_fg_chain,
  best_candles_fg_chain_ta,
  tuned_model
)

# Sort by accuracy (descending)
all_best_models <- all_best_models[order(-all_best_models$accuracy), ]

# Display the table
knitr::kable(all_best_models, 
             format = "simple", 
             caption = "Comparison of best models from each feature set and baseline methods",
             digits = 4,
             col.names = c("Model", "Features", "Model Type", "Lag", "Accuracy"))
```

While our best model has an accuracy of `0.5499`, we would still consider the fine tuned algorithm more robust with an accuracy of `0.5457`.

## Most relevant features

Now let's see what are the most important features of the algorithm.

```{r feature_importance}
variable_importance <- varImp(gbm_model_candles_fg_chain_ta_lag_1_tuned)
ggplot(variable_importance, aes(x = reorder(feature, Overall), y = Overall)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Variable Importance", x = "Feature", y = "Importance")
```
Interestingly the features with an importance higher than 25 are related to TA, Chain Data and Candles data.

## Confusion matrix

```{r confusion_matrix}
predicted_values <- predict(gbm_model_candles_fg_chain_ta_lag_1_tuned, test_set)
confusion_matrix <- confusionMatrix(factor(predicted_values, levels = c("up", "down")), 
                                   factor(test_set$direction, levels = c("up", "down")))
confusion_matrix
```
We can notice that the model is slightly better at predicting `down` than `up` at least with the `test_set`.
Which means that in in future trading it would potentially get a better success rate at shorting rather than longing.

# Conclusion

We have learned in this study that predictions using a trained model are better than luck.

With an accuracy of `0.5457` it would be important for a trader to use the predictions along with a well defined target for take-profit and stop-loss where the profit targeted should be higher than the stop-loss targeted.

Let's see what are the limitations of this study and what could be done next.

## Limitations

As mentioned in the report, most of the training have been done without cross-validation, in order to save computation time, therefore some other algorithm may have performed better than the current one.

## Potential improvements

A deeper study of the existing research could be used as a base to improve this algorithm, also there may be some other algorithms working even better than GBM that may be worth be trained.

Also, some other Technical Analysis indicator could be used to have better predictions maybe by coupling our hourly candles to a smaller time frame of candles.

Last but not least, using different algorithm depending on the type of market could also be a good solution. By comparing how algorithms performs in a different type of market, bearish, bullish or sideway and switching to the right model depending on the type of market could also improve the accuracy.

## Trading application

In order to be closer to the trading reality and test the ability of the model to make profit, I would recommend to start with backtesting to see how it performs setting good stop loss / take profit targets. Then after tuning the trading algorithm it would be worth doing some paper trading before doing actual real trading.










